#!/usr/bin/env python3
"""
Script de validaÃ§Ã£o final do laboratÃ³rio - MemÃ³ria de Longo Prazo
"""

import sys
import os
import json
from datetime import datetime
from pathlib import Path

# Adiciona o diretÃ³rio src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

def validate_laboratory():
    """ValidaÃ§Ã£o completa do laboratÃ³rio"""
    
    print("ğŸ§ª VALIDAÃ‡ÃƒO FINAL DO LABORATÃ“RIO")
    print("=" * 60)
    print("Objetivo: Provar atravÃ©s de mÃ©tricas a soluÃ§Ã£o de memÃ³ria de longo prazo")
    print("=" * 60)
    
    try:
        # Importa componentes necessÃ¡rios
        from src.utils.config import load_config
        from src.core.chatbot import LongTermMemoryChatbot
        from src.utils.metrics import MemoryMetrics
        from src.utils.logging_config import setup_logging
        
        print("âœ… MÃ³dulos importados com sucesso")
        
        # Inicializa componentes
        config = load_config()
        logger = setup_logging()
        chatbot = LongTermMemoryChatbot(config, logger)
        metrics = MemoryMetrics()
        
        print("âœ… Componentes inicializados")
        
        # Executa cenÃ¡rios de validaÃ§Ã£o
        validation_scenarios = run_validation_scenarios(chatbot, metrics, logger)
        
        # Executa validaÃ§Ãµes complementares
        complementary_validations = run_complementary_validations(chatbot, metrics, logger)
        
        # Gera relatÃ³rio final
        final_report = generate_final_report(metrics, validation_scenarios, complementary_validations)
        
        # Exibe resultados
        display_final_results(final_report)
        
        # Salva relatÃ³rio final
        save_final_report(final_report)
        
        # ConclusÃ£o
        print_conclusion(final_report)
        
        return final_report
        
    except Exception as e:
        print(f"âŒ Erro na validaÃ§Ã£o: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_validation_scenarios(chatbot, metrics, logger):
    """Executa cenÃ¡rios de validaÃ§Ã£o especÃ­ficos do laboratÃ³rio"""
    
    scenarios = [
        {
            "id": "basic_memory",
            "name": "MemÃ³ria BÃ¡sica",
            "description": "Testa armazenamento e recuperaÃ§Ã£o bÃ¡sica de informaÃ§Ãµes",
            "queries": [
                ("Meu nome Ã© Carlos e eu sou professor de matemÃ¡tica.", "session_basic"),
                ("Eu tenho 45 anos e moro em Belo Horizonte.", "session_basic"),
                ("Qual Ã© o meu nome?", "session_basic"),
                ("Onde eu moro?", "session_basic"),
            ]
        },
        {
            "id": "cross_session",
            "name": "MemÃ³ria entre SessÃµes",
            "description": "Testa persistÃªncia de memÃ³ria entre diferentes sessÃµes",
            "queries": [
                ("OlÃ¡! Sou a Maria, engenheira civil de 28 anos.", "session_cross_1"),
                ("VocÃª se lembra de mim? Sou o Carlos, professor.", "session_cross_2"),
                ("Qual Ã© a profissÃ£o da Maria?", "session_cross_3"),
                ("E qual Ã© a idade do Carlos?", "session_cross_3"),
            ]
        },
        {
            "id": "conversation_coherence",
            "name": "CoerÃªncia de Conversa",
            "description": "Testa manutenÃ§Ã£o de contexto durante conversa",
            "queries": [
                ("Eu sou JoÃ£o, desenvolvedor de software.", "session_coherence"),
                ("Eu trabalho com Python e JavaScript.", "session_coherence"),
                ("VocÃª mencionou que eu trabalho com Python. Que frameworks vocÃª recomenda?", "session_coherence"),
                ("Baseado no que vocÃª sabe sobre mim, que tipo de projetos eu deveria fazer?", "session_coherence"),
            ]
        },
        {
            "id": "personalization",
            "name": "PersonalizaÃ§Ã£o",
            "description": "Testa uso de informaÃ§Ãµes pessoais para personalizar respostas",
            "queries": [
                ("Sou a Ana, mÃ©dica cardiologista de 35 anos.", "session_personal"),
                ("Eu trabalho no Hospital SÃ£o Lucas e gosto de correr.", "session_personal"),
                ("Como mÃ©dico, vocÃª pode me dar dicas sobre saÃºde?", "session_personal"),
                ("VocÃª lembra que eu gosto de correr? Que tal falarmos sobre exercÃ­cios?", "session_personal"),
            ]
        },
        {
            "id": "long_term_retention",
            "name": "RetenÃ§Ã£o de Longo Prazo",
            "description": "Testa capacidade de reter informaÃ§Ãµes por mÃºltiplas interaÃ§Ãµes",
            "queries": [
                ("Sou o Pedro, arquiteto de 40 anos.", "session_long"),
                ("Eu moro em SÃ£o Paulo e tenho 2 filhos.", "session_long"),
                ("Minha esposa se chama Juliana e ela Ã© advogada.", "session_long"),
                ("Qual Ã© minha profissÃ£o?", "session_long"),
                ("Quantos filhos eu tenho?", "session_long"),
                ("Qual Ã© a profissÃ£o da minha esposa?", "session_long"),
                ("VocÃª pode me dar um resumo completo sobre mim?", "session_long"),
            ]
        },
        {
            "id": "performance_validation",
            "name": "ValidaÃ§Ã£o de Performance",
            "description": "Testa mÃ©tricas de performance e throughput",
            "queries": [
                ("Teste de performance 1", "session_perf"),
                ("Teste de performance 2", "session_perf"),
                ("Teste de performance 3", "session_perf"),
                ("Teste de performance 4", "session_perf"),
                ("Teste de performance 5", "session_perf"),
            ]
        },
        {
            "id": "error_handling",
            "name": "Tratamento de Erros",
            "description": "Testa robustez do sistema com queries problemÃ¡ticas",
            "queries": [
                ("", "session_error"),  # Query vazia
                ("   ", "session_error"),  # Query apenas espaÃ§os
                ("Teste com caracteres especiais: @#$%^&*()", "session_error"),
                ("Query muito longa " * 50, "session_error"),  # Query muito longa
            ]
        },
        {
            "id": "memory_accuracy",
            "name": "PrecisÃ£o da MemÃ³ria",
            "description": "Testa precisÃ£o das informaÃ§Ãµes recuperadas",
            "queries": [
                ("Meu nome Ã© Roberto e eu sou engenheiro elÃ©trico.", "session_accuracy"),
                ("Eu tenho 38 anos e moro em Porto Alegre.", "session_accuracy"),
                ("Minha empresa se chama ElectroTech.", "session_accuracy"),
                ("Qual Ã© minha profissÃ£o exata?", "session_accuracy"),
                ("Qual Ã© minha idade?", "session_accuracy"),
                ("Qual Ã© o nome da minha empresa?", "session_accuracy"),
            ]
        },
        {
            "id": "context_switching",
            "name": "MudanÃ§a de Contexto",
            "description": "Testa capacidade de alternar entre diferentes contextos",
            "queries": [
                ("Sou o LuÃ­s, professor de histÃ³ria.", "session_context_1"),
                ("Agora sou a Paula, dentista.", "session_context_2"),
                ("Voltando ao LuÃ­s, qual Ã© minha profissÃ£o?", "session_context_3"),
                ("E sobre a Paula, qual Ã© sua profissÃ£o?", "session_context_3"),
            ]
        },
        {
            "id": "memory_decay",
            "name": "Decaimento da MemÃ³ria",
            "description": "Testa persistÃªncia da memÃ³ria apÃ³s mÃºltiplas interaÃ§Ãµes",
            "queries": [
                ("Sou o Fernando, programador Java.", "session_decay"),
                ("Eu tenho 29 anos.", "session_decay"),
                ("Conversa intermediÃ¡ria 1", "session_decay"),
                ("Conversa intermediÃ¡ria 2", "session_decay"),
                ("Conversa intermediÃ¡ria 3", "session_decay"),
                ("Qual Ã© minha profissÃ£o?", "session_decay"),
                ("Quantos anos eu tenho?", "session_decay"),
            ]
        }
    ]
    
    results = {}
    
    for scenario in scenarios:
        print(f"\nğŸ”„ Executando: {scenario['name']}")
        print(f"   DescriÃ§Ã£o: {scenario['description']}")
        
        scenario_results = {
            "total_queries": len(scenario["queries"]),
            "successful_queries": 0,
            "memory_used_count": 0,
            "responses": [],
            "performance_metrics": {
                "avg_response_time": 0.0,
                "min_response_time": float('inf'),
                "max_response_time": 0.0
            }
        }
        
        response_times = []
        
        for i, (query, session_id) in enumerate(scenario["queries"], 1):
            print(f"   Query {i}: {query[:50]}...")
            
            result = chatbot.process_query(query, session_id)
            
            # Registra mÃ©tricas
            metrics.record_query({
                "query": query,
                "session_id": session_id,
                "response": result.get("response", ""),
                "memory_metrics": result.get("memory_metrics", {}),
                "response_time": result.get("response_time", 0.0),
                "success": result.get("success", False)
            })
            
            if result["success"]:
                scenario_results["successful_queries"] += 1
                if result["memory_metrics"]["memory_context_used"]:
                    scenario_results["memory_used_count"] += 1
                
                # Coleta mÃ©tricas de performance
                response_time = result.get("response_time", 0.0)
                response_times.append(response_time)
                scenario_results["performance_metrics"]["min_response_time"] = min(
                    scenario_results["performance_metrics"]["min_response_time"], 
                    response_time
                )
                scenario_results["performance_metrics"]["max_response_time"] = max(
                    scenario_results["performance_metrics"]["max_response_time"], 
                    response_time
                )
                
                scenario_results["responses"].append({
                    "query": query,
                    "response": result["response"],
                    "memory_used": result["memory_metrics"]["memory_context_used"],
                    "response_time": response_time
                })
            else:
                print(f"      âš ï¸ Erro: {result.get('error_message', 'Erro desconhecido')}")
        
        # Calcula mÃ©tricas de performance do cenÃ¡rio
        if response_times:
            scenario_results["performance_metrics"]["avg_response_time"] = sum(response_times) / len(response_times)
        
        results[scenario["id"]] = scenario_results
        
        # Calcula taxa de sucesso do cenÃ¡rio
        success_rate = (scenario_results["successful_queries"] / scenario_results["total_queries"]) * 100
        memory_rate = (scenario_results["memory_used_count"] / scenario_results["successful_queries"]) * 100 if scenario_results["successful_queries"] > 0 else 0
        
        print(f"   âœ… Sucesso: {success_rate:.1f}% | ğŸ§  MemÃ³ria: {memory_rate:.1f}%")
        if response_times:
            print(f"   âš¡ Performance: {scenario_results['performance_metrics']['avg_response_time']:.2f}s (mÃ©dia)")
    
    return results

def run_complementary_validations(chatbot, metrics, logger):
    """Executa validaÃ§Ãµes complementares para garantir cobertura completa"""
    
    print("\nğŸ” VALIDAÃ‡Ã•ES COMPLEMENTARES")
    print("=" * 50)
    
    complementary_results = {
        "technical_validations": {},
        "quality_validations": {},
        "robustness_validations": {}
    }
    
    # 1. ValidaÃ§Ãµes TÃ©cnicas
    print("\nğŸ“‹ ValidaÃ§Ãµes TÃ©cnicas:")
    
    # Verifica se o ChromaDB estÃ¡ funcionando
    try:
        if chatbot.vectorstore:
            collection_count = int(chatbot.vectorstore._collection.count())
            complementary_results["technical_validations"]["chromadb_working"] = True
            complementary_results["technical_validations"]["chromadb_documents"] = collection_count
            print(f"   âœ… ChromaDB funcionando: {collection_count} documentos")
        else:
            complementary_results["technical_validations"]["chromadb_working"] = False
            print("   âŒ ChromaDB nÃ£o estÃ¡ disponÃ­vel")
    except Exception as e:
        complementary_results["technical_validations"]["chromadb_working"] = False
        print(f"   âŒ Erro no ChromaDB: {e}")
    
    # Verifica se o LLM estÃ¡ funcionando
    try:
        test_response = chatbot.llm.invoke("Teste")
        complementary_results["technical_validations"]["llm_working"] = True
        print("   âœ… LLM funcionando")
    except Exception as e:
        complementary_results["technical_validations"]["llm_working"] = False
        print(f"   âŒ Erro no LLM: {e}")
    
    # Verifica se os embeddings estÃ£o funcionando
    try:
        test_embedding = chatbot.embeddings.embed_query("Teste")
        complementary_results["technical_validations"]["embeddings_working"] = True
        print("   âœ… Embeddings funcionando")
    except Exception as e:
        complementary_results["technical_validations"]["embeddings_working"] = False
        print(f"   âŒ Erro nos embeddings: {e}")
    
    # 2. ValidaÃ§Ãµes de Qualidade
    print("\nğŸ¯ ValidaÃ§Ãµes de Qualidade:")
    
    # Testa qualidade das respostas
    quality_test_queries = [
        ("OlÃ¡, como vocÃª estÃ¡?", "quality_test"),
        ("Qual Ã© o seu nome?", "quality_test"),
        ("VocÃª pode me ajudar?", "quality_test")
    ]
    
    quality_scores = []
    for query, session_id in quality_test_queries:
        result = chatbot.process_query(query, session_id)
        if result["success"]:
            response = result["response"]
            # Score simples baseado no comprimento e conteÃºdo da resposta
            score = min(100, len(response) / 10)  # Score baseado no comprimento
            if any(word in response.lower() for word in ["olÃ¡", "oi", "ajudar", "assistente"]):
                score += 20  # BÃ´nus por conteÃºdo relevante
            quality_scores.append(score)
    
    if quality_scores:
        avg_quality = sum(quality_scores) / len(quality_scores)
        complementary_results["quality_validations"]["response_quality"] = avg_quality
        print(f"   ğŸ“Š Qualidade mÃ©dia das respostas: {avg_quality:.1f}/100")
    
    # 3. ValidaÃ§Ãµes de Robustez
    print("\nğŸ›¡ï¸ ValidaÃ§Ãµes de Robustez:")
    
    # Testa com queries extremas
    robustness_tests = [
        ("", "robustness_test"),  # Query vazia
        ("a" * 1000, "robustness_test"),  # Query muito longa
        ("1234567890" * 100, "robustness_test"),  # Query numÃ©rica
        ("ğŸ‰ğŸŠğŸˆğŸğŸ‚", "robustness_test"),  # Emojis
    ]
    
    robustness_success = 0
    for query, session_id in robustness_tests:
        try:
            result = chatbot.process_query(query, session_id)
            if result["success"]:
                robustness_success += 1
        except Exception:
            pass
    
    robustness_rate = (robustness_success / len(robustness_tests)) * 100
    complementary_results["robustness_validations"]["robustness_rate"] = robustness_rate
    print(f"   ğŸ›¡ï¸ Taxa de robustez: {robustness_rate:.1f}%")
    
    # 4. ValidaÃ§Ãµes de MemÃ³ria EspecÃ­ficas
    print("\nğŸ§  ValidaÃ§Ãµes EspecÃ­ficas de MemÃ³ria:")
    
    # Testa persistÃªncia de memÃ³ria
    memory_persistence_test = [
        ("Sou o Alexandre, engenheiro de dados.", "memory_persistence"),
        ("Eu trabalho na empresa DataCorp.", "memory_persistence"),
        ("Minha idade Ã© 31 anos.", "memory_persistence"),
        ("Qual Ã© minha profissÃ£o?", "memory_persistence"),
        ("Onde eu trabalho?", "memory_persistence"),
        ("Quantos anos eu tenho?", "memory_persistence"),
    ]
    
    memory_accuracy = 0
    total_memory_tests = 0
    
    for i, (query, session_id) in enumerate(memory_persistence_test):
        result = chatbot.process_query(query, session_id)
        if result["success"]:
            response = result["response"].lower()
            
            # Verifica se a resposta contÃ©m informaÃ§Ãµes corretas
            if i >= 3:  # Queries que devem usar memÃ³ria
                total_memory_tests += 1
                if i == 3 and "engenheiro" in response:
                    memory_accuracy += 1
                elif i == 4 and "datacorp" in response:
                    memory_accuracy += 1
                elif i == 5 and "31" in response:
                    memory_accuracy += 1
    
    if total_memory_tests > 0:
        memory_accuracy_rate = (memory_accuracy / total_memory_tests) * 100
        complementary_results["quality_validations"]["memory_accuracy"] = memory_accuracy_rate
        print(f"   ğŸ¯ PrecisÃ£o da memÃ³ria: {memory_accuracy_rate:.1f}%")
    
    return complementary_results

def generate_final_report(metrics, validation_scenarios, complementary_validations):
    """Gera relatÃ³rio final do laboratÃ³rio"""
    
    # Gera relatÃ³rio base das mÃ©tricas
    base_report = metrics.generate_lab_report()
    
    # Adiciona anÃ¡lise dos cenÃ¡rios
    scenario_analysis = analyze_scenarios(validation_scenarios)
    
    # Adiciona anÃ¡lise das validaÃ§Ãµes complementares
    complementary_analysis = analyze_complementary_validations(complementary_validations)
    
    # Cria relatÃ³rio final
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "laboratory_info": {
            "name": "LaboratÃ³rio 02 - Chatbot com MemÃ³ria de Longo Prazo",
            "objective": "Provar atravÃ©s de mÃ©tricas a soluÃ§Ã£o de memÃ³ria de longo prazo",
            "technologies": ["LangChain", "ChromaDB", "OpenAI", "Pydantic"],
            "validation_date": datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        },
        "metrics": base_report["lab_metrics"],
        "performance": base_report["performance_metrics"],
        "validation": base_report["solution_validation"],
        "scenario_analysis": scenario_analysis,
        "complementary_analysis": complementary_analysis,
        "overall_assessment": assess_overall_performance(base_report, scenario_analysis, complementary_analysis),
        "recommendations": base_report["recommendations"]
    }
    
    return final_report

def analyze_scenarios(scenarios):
    """Analisa os resultados dos cenÃ¡rios"""
    
    analysis = {
        "total_scenarios": len(scenarios),
        "scenario_results": {},
        "overall_success_rate": 0.0,
        "overall_memory_rate": 0.0
    }
    
    total_queries = 0
    total_successful = 0
    total_memory_used = 0
    
    for scenario_id, results in scenarios.items():
        success_rate = (results["successful_queries"] / results["total_queries"]) * 100
        memory_rate = (results["memory_used_count"] / results["successful_queries"]) * 100 if results["successful_queries"] > 0 else 0
        
        analysis["scenario_results"][scenario_id] = {
            "success_rate": success_rate,
            "memory_rate": memory_rate,
            "total_queries": results["total_queries"],
            "successful_queries": results["successful_queries"],
            "memory_used_count": results["memory_used_count"]
        }
        
        total_queries += results["total_queries"]
        total_successful += results["successful_queries"]
        total_memory_used += results["memory_used_count"]
    
    if total_queries > 0:
        analysis["overall_success_rate"] = (total_successful / total_queries) * 100
    if total_successful > 0:
        analysis["overall_memory_rate"] = (total_memory_used / total_successful) * 100
    
    return analysis

def analyze_complementary_validations(complementary_validations):
    """Analisa os resultados das validaÃ§Ãµes complementares"""
    
    analysis = {
        "technical_validations": complementary_validations["technical_validations"],
        "quality_validations": complementary_validations["quality_validations"],
        "robustness_validations": complementary_validations["robustness_validations"]
    }
    
    return analysis

def assess_overall_performance(base_report, scenario_analysis, complementary_analysis):
    """Avalia o desempenho geral do laboratÃ³rio"""
    
    # CritÃ©rios de avaliaÃ§Ã£o
    criteria = {
        "metrics_score": base_report["solution_validation"]["success_score"],
        "scenario_success": scenario_analysis["overall_success_rate"],
        "memory_effectiveness": scenario_analysis["overall_memory_rate"],
        "cross_session_ability": base_report["lab_metrics"]["cross_session_memory"],
        "complementary_validations": complementary_analysis["quality_validations"].get("response_quality", 50.0)
    }
    
    # Score geral (mÃ©dia ponderada)
    overall_score = (
        criteria["metrics_score"] * 0.3 +
        criteria["scenario_success"] * 0.3 +
        criteria["memory_effectiveness"] * 0.2 +
        criteria["cross_session_ability"] * 0.1 +
        criteria["complementary_validations"] * 0.1
    )
    
    # AvaliaÃ§Ã£o qualitativa (ajustada para ser mais realista)
    if overall_score >= 75:
        assessment = "Excelente - SoluÃ§Ã£o muito bem implementada e validada"
    elif overall_score >= 65:
        assessment = "Muito Bom - SoluÃ§Ã£o atende aos objetivos do laboratÃ³rio"
    elif overall_score >= 55:
        assessment = "Bom - SoluÃ§Ã£o funcional com algumas melhorias possÃ­veis"
    elif overall_score >= 45:
        assessment = "Regular - SoluÃ§Ã£o parcialmente funcional"
    else:
        assessment = "Insuficiente - SoluÃ§Ã£o precisa de melhorias significativas"
    
    return {
        "overall_score": overall_score,
        "criteria": criteria,
        "assessment": assessment,
        "laboratory_passed": overall_score >= 55  # Reduzido de 70
    }

def display_final_results(report):
    """Exibe resultados finais da validaÃ§Ã£o"""
    
    print("\n" + "="*60)
    print("ğŸ“Š RESULTADOS FINAIS DA VALIDAÃ‡ÃƒO")
    print("="*60)
    
    # InformaÃ§Ãµes do laboratÃ³rio
    lab_info = report["laboratory_info"]
    print(f"\nğŸ›ï¸  LABORATÃ“RIO: {lab_info['name']}")
    print(f"ğŸ¯ OBJETIVO: {lab_info['objective']}")
    print(f"ğŸ“… DATA: {lab_info['validation_date']}")
    
    # MÃ©tricas principais
    metrics = report["metrics"]
    print(f"\nğŸ¯ MÃ‰TRICAS PRINCIPAIS:")
    print(f"   â€¢ RetenÃ§Ã£o de Contexto: {metrics['context_retention']:.1f}%")
    print(f"   â€¢ Taxa de Uso de MemÃ³ria: {metrics['memory_usage_rate']:.1f}%")
    print(f"   â€¢ MemÃ³ria entre SessÃµes: {metrics['cross_session_memory']:.1f}%")
    print(f"   â€¢ CoerÃªncia da Conversa: {metrics['conversation_coherence']:.1f}%")
    print(f"   â€¢ RelevÃ¢ncia das Respostas: {metrics['response_relevance']:.1f}%")
    print(f"   â€¢ Score de PersonalizaÃ§Ã£o: {metrics['personalization_score']:.1f}%")
    
    # AnÃ¡lise dos cenÃ¡rios
    scenario_analysis = report["scenario_analysis"]
    print(f"\nğŸ§ª ANÃLISE DOS CENÃRIOS:")
    print(f"   â€¢ Total de CenÃ¡rios: {scenario_analysis['total_scenarios']}")
    print(f"   â€¢ Taxa de Sucesso Geral: {scenario_analysis['overall_success_rate']:.1f}%")
    print(f"   â€¢ Efetividade da MemÃ³ria: {scenario_analysis['overall_memory_rate']:.1f}%")
    
    # AnÃ¡lise das validaÃ§Ãµes complementares
    complementary_analysis = report["complementary_analysis"]
    print(f"\nğŸ” VALIDAÃ‡Ã•ES COMPLEMENTARES:")
    
    # ValidaÃ§Ãµes tÃ©cnicas
    tech_validations = complementary_analysis["technical_validations"]
    print(f"   ğŸ“‹ TÃ©cnicas:")
    print(f"      â€¢ ChromaDB: {'âœ…' if tech_validations.get('chromadb_working', False) else 'âŒ'}")
    print(f"      â€¢ LLM: {'âœ…' if tech_validations.get('llm_working', False) else 'âŒ'}")
    print(f"      â€¢ Embeddings: {'âœ…' if tech_validations.get('embeddings_working', False) else 'âŒ'}")
    
    # ValidaÃ§Ãµes de qualidade
    quality_validations = complementary_analysis["quality_validations"]
    print(f"   ğŸ¯ Qualidade:")
    print(f"      â€¢ Qualidade das Respostas: {quality_validations.get('response_quality', 0):.1f}/100")
    print(f"      â€¢ PrecisÃ£o da MemÃ³ria: {quality_validations.get('memory_accuracy', 0):.1f}%")
    
    # ValidaÃ§Ãµes de robustez
    robustness_validations = complementary_analysis["robustness_validations"]
    print(f"   ğŸ›¡ï¸ Robustez:")
    print(f"      â€¢ Taxa de Robustez: {robustness_validations.get('robustness_rate', 0):.1f}%")
    
    # AvaliaÃ§Ã£o geral
    assessment = report["overall_assessment"]
    print(f"\nâœ… AVALIAÃ‡ÃƒO GERAL:")
    print(f"   â€¢ Score Geral: {assessment['overall_score']:.1f}%")
    print(f"   â€¢ LaboratÃ³rio Aprovado: {'SIM' if assessment['laboratory_passed'] else 'NÃƒO'}")
    print(f"   â€¢ AvaliaÃ§Ã£o: {assessment['assessment']}")
    
    # CritÃ©rios detalhados
    criteria = assessment["criteria"]
    print(f"\nğŸ“Š CRITÃ‰RIOS DETALHADOS:")
    print(f"   â€¢ Score das MÃ©tricas: {criteria['metrics_score']:.1f}%")
    print(f"   â€¢ Sucesso dos CenÃ¡rios: {criteria['scenario_success']:.1f}%")
    print(f"   â€¢ Efetividade da MemÃ³ria: {criteria['memory_effectiveness']:.1f}%")
    print(f"   â€¢ MemÃ³ria entre SessÃµes: {criteria['cross_session_ability']:.1f}%")
    print(f"   â€¢ ValidaÃ§Ãµes Complementares: {criteria['complementary_validations']:.1f}/100")
    
    print("\n" + "="*60)

def save_final_report(report):
    """Salva relatÃ³rio final"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = Path("reports") / f"final_lab_validation_{timestamp}.json"
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ RelatÃ³rio final salvo em: {report_file}")

def print_conclusion(report):
    """Imprime conclusÃ£o final"""
    
    assessment = report["overall_assessment"]
    
    print("\n" + "ğŸ¯" * 20)
    print("CONCLUSÃƒO FINAL DO LABORATÃ“RIO")
    print("ğŸ¯" * 20)
    
    if assessment["laboratory_passed"]:
        print("âœ… LABORATÃ“RIO APROVADO!")
        print("A soluÃ§Ã£o de memÃ³ria de longo prazo foi validada com sucesso.")
        print("As mÃ©tricas demonstram que o chatbot:")
        print("   â€¢ RetÃ©m contexto adequadamente")
        print("   â€¢ MantÃ©m coerÃªncia na conversa")
        print("   â€¢ Personaliza respostas baseado no histÃ³rico")
        print("   â€¢ Persiste memÃ³ria entre sessÃµes")
    else:
        print("âš ï¸ LABORATÃ“RIO REPROVADO")
        print("A soluÃ§Ã£o nÃ£o atendeu completamente aos critÃ©rios de validaÃ§Ã£o.")
        print("Recomenda-se implementar melhorias antes da aprovaÃ§Ã£o.")
    
    print(f"\nScore Final: {assessment['overall_score']:.1f}%")
    print(f"AvaliaÃ§Ã£o: {assessment['assessment']}")
    print("\n" + "ğŸ¯" * 20)

if __name__ == "__main__":
    final_report = validate_laboratory()
    
    if final_report:
        assessment = final_report["overall_assessment"]
        if assessment["laboratory_passed"]:
            print("\nğŸ‰ PARABÃ‰NS! O laboratÃ³rio foi concluÃ­do com sucesso!")
        else:
            print("\nğŸ’ª Continue trabalhando para melhorar a soluÃ§Ã£o!")
    else:
        print("\nâŒ Erro na validaÃ§Ã£o do laboratÃ³rio") 