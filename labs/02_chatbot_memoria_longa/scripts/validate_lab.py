#!/usr/bin/env python3
"""
Script de valida√ß√£o final do laborat√≥rio - Mem√≥ria de Longo Prazo
"""

import sys
import os
import json
from datetime import datetime
from pathlib import Path

# Adiciona o diret√≥rio src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

def validate_laboratory():
    """Valida√ß√£o completa do laborat√≥rio"""
    
    print("üß™ VALIDA√á√ÉO FINAL DO LABORAT√ìRIO")
    print("=" * 60)
    print("Objetivo: Provar atrav√©s de m√©tricas a solu√ß√£o de mem√≥ria de longo prazo")
    print("=" * 60)
    
    try:
        # Importa componentes necess√°rios
        from src.utils.config import load_config
        from src.core.chatbot import LongTermMemoryChatbot
        from src.utils.metrics import MemoryMetrics
        from src.utils.logging_config import setup_logging
        
        print("‚úÖ M√≥dulos importados com sucesso")
        
        # Inicializa componentes
        config = load_config()
        logger = setup_logging()
        chatbot = LongTermMemoryChatbot(config, logger)
        metrics = MemoryMetrics()
        
        print("‚úÖ Componentes inicializados")
        
        # Executa cen√°rios de valida√ß√£o
        validation_scenarios = run_validation_scenarios(chatbot, metrics, logger)
        
        # Executa valida√ß√µes complementares
        complementary_validations = run_complementary_validations(chatbot, metrics, logger)
        
        # Gera relat√≥rio final
        final_report = generate_final_report(metrics, validation_scenarios, complementary_validations)
        
        # Exibe resultados
        display_final_results(final_report)
        
        # Salva relat√≥rio final
        save_final_report(final_report)
        
        # Conclus√£o
        print_conclusion(final_report)
        
        return final_report
        
    except Exception as e:
        print(f"‚ùå Erro na valida√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_validation_scenarios(chatbot, metrics, logger):
    """Executa cen√°rios de valida√ß√£o espec√≠ficos do laborat√≥rio"""
    
    scenarios = [
        {
            "id": "basic_memory",
            "name": "Mem√≥ria B√°sica",
            "description": "Testa armazenamento e recupera√ß√£o b√°sica de informa√ß√µes",
            "queries": [
                ("Meu nome √© Carlos e eu sou professor de matem√°tica.", "session_basic"),
                ("Eu tenho 45 anos e moro em Belo Horizonte.", "session_basic"),
                ("Qual √© o meu nome?", "session_basic"),
                ("Onde eu moro?", "session_basic"),
            ]
        },
        {
            "id": "cross_session",
            "name": "Mem√≥ria entre Sess√µes",
            "description": "Testa persist√™ncia de mem√≥ria entre diferentes sess√µes",
            "queries": [
                ("Ol√°! Sou a Maria, engenheira civil de 28 anos.", "session_cross_1"),
                ("Voc√™ se lembra de mim? Sou o Carlos, professor.", "session_cross_2"),
                ("Qual √© a profiss√£o da Maria?", "session_cross_3"),
                ("E qual √© a idade do Carlos?", "session_cross_3"),
            ]
        },
        {
            "id": "conversation_coherence",
            "name": "Coer√™ncia de Conversa",
            "description": "Testa manuten√ß√£o de contexto durante conversa",
            "queries": [
                ("Eu sou Jo√£o, desenvolvedor de software.", "session_coherence"),
                ("Eu trabalho com Python e JavaScript.", "session_coherence"),
                ("Voc√™ mencionou que eu trabalho com Python. Que frameworks voc√™ recomenda?", "session_coherence"),
                ("Baseado no que voc√™ sabe sobre mim, que tipo de projetos eu deveria fazer?", "session_coherence"),
            ]
        },
        {
            "id": "personalization",
            "name": "Personaliza√ß√£o",
            "description": "Testa uso de informa√ß√µes pessoais para personalizar respostas",
            "queries": [
                ("Sou a Ana, m√©dica cardiologista de 35 anos.", "session_personal"),
                ("Eu trabalho no Hospital S√£o Lucas e gosto de correr.", "session_personal"),
                ("Como m√©dico, voc√™ pode me dar dicas sobre sa√∫de?", "session_personal"),
                ("Voc√™ lembra que eu gosto de correr? Que tal falarmos sobre exerc√≠cios?", "session_personal"),
            ]
        },
        {
            "id": "long_term_retention",
            "name": "Reten√ß√£o de Longo Prazo",
            "description": "Testa capacidade de reter informa√ß√µes por m√∫ltiplas intera√ß√µes",
            "queries": [
                ("Sou o Pedro, arquiteto de 40 anos.", "session_long"),
                ("Eu moro em S√£o Paulo e tenho 2 filhos.", "session_long"),
                ("Minha esposa se chama Juliana e ela √© advogada.", "session_long"),
                ("Qual √© minha profiss√£o?", "session_long"),
                ("Quantos filhos eu tenho?", "session_long"),
                ("Qual √© a profiss√£o da minha esposa?", "session_long"),
                ("Voc√™ pode me dar um resumo completo sobre mim?", "session_long"),
            ]
        },
        {
            "id": "performance_validation",
            "name": "Valida√ß√£o de Performance",
            "description": "Testa m√©tricas de performance e throughput",
            "queries": [
                ("Teste de performance 1", "session_perf"),
                ("Teste de performance 2", "session_perf"),
                ("Teste de performance 3", "session_perf"),
                ("Teste de performance 4", "session_perf"),
                ("Teste de performance 5", "session_perf"),
            ]
        },
        {
            "id": "error_handling",
            "name": "Tratamento de Erros",
            "description": "Testa robustez do sistema com queries problem√°ticas",
            "queries": [
                ("", "session_error"),  # Query vazia
                ("   ", "session_error"),  # Query apenas espa√ßos
                ("Teste com caracteres especiais: @#$%^&*()", "session_error"),
                ("Query muito longa " * 50, "session_error"),  # Query muito longa
            ]
        },
        {
            "id": "memory_accuracy",
            "name": "Precis√£o da Mem√≥ria",
            "description": "Testa precis√£o das informa√ß√µes recuperadas",
            "queries": [
                ("Meu nome √© Roberto e eu sou engenheiro el√©trico.", "session_accuracy"),
                ("Eu tenho 38 anos e moro em Porto Alegre.", "session_accuracy"),
                ("Minha empresa se chama ElectroTech.", "session_accuracy"),
                ("Qual √© minha profiss√£o exata?", "session_accuracy"),
                ("Qual √© minha idade?", "session_accuracy"),
                ("Qual √© o nome da minha empresa?", "session_accuracy"),
            ]
        },
        {
            "id": "context_switching",
            "name": "Mudan√ßa de Contexto",
            "description": "Testa capacidade de alternar entre diferentes contextos",
            "queries": [
                ("Sou o Lu√≠s, professor de hist√≥ria.", "session_context_1"),
                ("Agora sou a Paula, dentista.", "session_context_2"),
                ("Voltando ao Lu√≠s, qual √© minha profiss√£o?", "session_context_3"),
                ("E sobre a Paula, qual √© sua profiss√£o?", "session_context_3"),
            ]
        },
        {
            "id": "memory_decay",
            "name": "Decaimento da Mem√≥ria",
            "description": "Testa persist√™ncia da mem√≥ria ap√≥s m√∫ltiplas intera√ß√µes",
            "queries": [
                ("Sou o Fernando, programador Java.", "session_decay"),
                ("Eu tenho 29 anos.", "session_decay"),
                ("Conversa intermedi√°ria 1", "session_decay"),
                ("Conversa intermedi√°ria 2", "session_decay"),
                ("Conversa intermedi√°ria 3", "session_decay"),
                ("Qual √© minha profiss√£o?", "session_decay"),
                ("Quantos anos eu tenho?", "session_decay"),
            ]
        }
    ]
    
    results = {}
    
    for scenario in scenarios:
        print(f"\nüîÑ Executando: {scenario['name']}")
        print(f"   Descri√ß√£o: {scenario['description']}")
        
        scenario_results = {
            "total_queries": len(scenario["queries"]),
            "successful_queries": 0,
            "memory_used_count": 0,
            "responses": [],
            "performance_metrics": {
                "avg_response_time": 0.0,
                "min_response_time": float('inf'),
                "max_response_time": 0.0
            }
        }
        
        response_times = []
        
        for i, (query, session_id) in enumerate(scenario["queries"], 1):
            print(f"   Query {i}: {query[:50]}...")
            
            result = chatbot.process_query(query, session_id)
            
            # Registra m√©tricas
            metrics.record_query({
                "query": query,
                "session_id": session_id,
                "response": result.get("response", ""),
                "memory_metrics": result.get("memory_metrics", {}),
                "response_time": result.get("response_time", 0.0),
                "success": result.get("success", False)
            })
            
            if result["success"]:
                scenario_results["successful_queries"] += 1
                if result["memory_metrics"]["memory_context_used"]:
                    scenario_results["memory_used_count"] += 1
                
                # Coleta m√©tricas de performance
                response_time = result.get("response_time", 0.0)
                response_times.append(response_time)
                scenario_results["performance_metrics"]["min_response_time"] = min(
                    scenario_results["performance_metrics"]["min_response_time"], 
                    response_time
                )
                scenario_results["performance_metrics"]["max_response_time"] = max(
                    scenario_results["performance_metrics"]["max_response_time"], 
                    response_time
                )
                
                scenario_results["responses"].append({
                    "query": query,
                    "response": result["response"],
                    "memory_used": result["memory_metrics"]["memory_context_used"],
                    "response_time": response_time
                })
            else:
                print(f"      ‚ö†Ô∏è Erro: {result.get('error_message', 'Erro desconhecido')}")
        
        # Calcula m√©tricas de performance do cen√°rio
        if response_times:
            scenario_results["performance_metrics"]["avg_response_time"] = sum(response_times) / len(response_times)
        
        results[scenario["id"]] = scenario_results
        
        # Calcula taxa de sucesso do cen√°rio
        success_rate = (scenario_results["successful_queries"] / scenario_results["total_queries"]) * 100
        memory_rate = (scenario_results["memory_used_count"] / scenario_results["successful_queries"]) * 100 if scenario_results["successful_queries"] > 0 else 0
        
        print(f"   ‚úÖ Sucesso: {success_rate:.1f}% | üß† Mem√≥ria: {memory_rate:.1f}%")
        if response_times:
            print(f"   ‚ö° Performance: {scenario_results['performance_metrics']['avg_response_time']:.2f}s (m√©dia)")
    
    return results

def run_complementary_validations(chatbot, metrics, logger):
    """Executa valida√ß√µes complementares para garantir cobertura completa"""
    
    print("\nüîç VALIDA√á√ïES COMPLEMENTARES")
    print("=" * 50)
    
    complementary_results = {
        "technical_validations": {},
        "quality_validations": {},
        "robustness_validations": {}
    }
    
    # 1. Valida√ß√µes T√©cnicas
    print("\nüìã Valida√ß√µes T√©cnicas:")
    
    # Verifica se o ChromaDB est√° funcionando
    try:
        if chatbot.vectorstore:
            collection_count = int(chatbot.vectorstore._collection.count())
            complementary_results["technical_validations"]["chromadb_working"] = True
            complementary_results["technical_validations"]["chromadb_documents"] = collection_count
            print(f"   ‚úÖ ChromaDB funcionando: {collection_count} documentos")
        else:
            complementary_results["technical_validations"]["chromadb_working"] = False
            print("   ‚ùå ChromaDB n√£o est√° dispon√≠vel")
    except Exception as e:
        complementary_results["technical_validations"]["chromadb_working"] = False
        print(f"   ‚ùå Erro no ChromaDB: {e}")
    
    # Verifica se o LLM est√° funcionando
    try:
        test_response = chatbot.llm.invoke("Teste")
        complementary_results["technical_validations"]["llm_working"] = True
        print("   ‚úÖ LLM funcionando")
    except Exception as e:
        complementary_results["technical_validations"]["llm_working"] = False
        print(f"   ‚ùå Erro no LLM: {e}")
    
    # Verifica se os embeddings est√£o funcionando
    try:
        test_embedding = chatbot.embeddings.embed_query("Teste")
        complementary_results["technical_validations"]["embeddings_working"] = True
        print("   ‚úÖ Embeddings funcionando")
    except Exception as e:
        complementary_results["technical_validations"]["embeddings_working"] = False
        print(f"   ‚ùå Erro nos embeddings: {e}")
    
    # 2. Valida√ß√µes de Qualidade
    print("\nüéØ Valida√ß√µes de Qualidade:")
    
    # Testa qualidade das respostas
    quality_test_queries = [
        ("Ol√°, como voc√™ est√°?", "quality_test"),
        ("Qual √© o seu nome?", "quality_test"),
        ("Voc√™ pode me ajudar?", "quality_test")
    ]
    
    quality_scores = []
    for query, session_id in quality_test_queries:
        result = chatbot.process_query(query, session_id)
        if result["success"]:
            response = result["response"]
            # Score simples baseado no comprimento e conte√∫do da resposta
            score = min(100, len(response) / 10)  # Score baseado no comprimento
            if any(word in response.lower() for word in ["ol√°", "oi", "ajudar", "assistente"]):
                score += 20  # B√¥nus por conte√∫do relevante
            quality_scores.append(score)
    
    if quality_scores:
        avg_quality = sum(quality_scores) / len(quality_scores)
        complementary_results["quality_validations"]["response_quality"] = avg_quality
        print(f"   üìä Qualidade m√©dia das respostas: {avg_quality:.1f}/100")
    
    # 3. Valida√ß√µes de Robustez
    print("\nüõ°Ô∏è Valida√ß√µes de Robustez:")
    
    # Testa com queries extremas
    robustness_tests = [
        ("", "robustness_test"),  # Query vazia
        ("a" * 1000, "robustness_test"),  # Query muito longa
        ("1234567890" * 100, "robustness_test"),  # Query num√©rica
        ("üéâüéäüéàüéÅüéÇ", "robustness_test"),  # Emojis
    ]
    
    robustness_success = 0
    for query, session_id in robustness_tests:
        try:
            result = chatbot.process_query(query, session_id)
            if result["success"]:
                robustness_success += 1
        except Exception:
            pass
    
    robustness_rate = (robustness_success / len(robustness_tests)) * 100
    complementary_results["robustness_validations"]["robustness_rate"] = robustness_rate
    print(f"   üõ°Ô∏è Taxa de robustez: {robustness_rate:.1f}%")
    
    # 4. Valida√ß√µes de Mem√≥ria Espec√≠ficas
    print("\nüß† Valida√ß√µes Espec√≠ficas de Mem√≥ria:")
    
    # Testa persist√™ncia de mem√≥ria
    memory_persistence_test = [
        ("Sou o Alexandre, engenheiro de dados.", "memory_persistence"),
        ("Eu trabalho na empresa DataCorp.", "memory_persistence"),
        ("Minha idade √© 31 anos.", "memory_persistence"),
        ("Qual √© minha profiss√£o?", "memory_persistence"),
        ("Onde eu trabalho?", "memory_persistence"),
        ("Quantos anos eu tenho?", "memory_persistence"),
    ]
    
    memory_accuracy = 0
    total_memory_tests = 0
    
    for i, (query, session_id) in enumerate(memory_persistence_test):
        result = chatbot.process_query(query, session_id)
        if result["success"]:
            response = result["response"].lower()
            
            # Verifica se a resposta cont√©m informa√ß√µes corretas
            if i >= 3:  # Queries que devem usar mem√≥ria
                total_memory_tests += 1
                if i == 3 and "engenheiro" in response:
                    memory_accuracy += 1
                elif i == 4 and "datacorp" in response:
                    memory_accuracy += 1
                elif i == 5 and "31" in response:
                    memory_accuracy += 1
    
    if total_memory_tests > 0:
        memory_accuracy_rate = (memory_accuracy / total_memory_tests) * 100
        complementary_results["quality_validations"]["memory_accuracy"] = memory_accuracy_rate
        print(f"   üéØ Precis√£o da mem√≥ria: {memory_accuracy_rate:.1f}%")
    
    return complementary_results

def generate_final_report(metrics, validation_scenarios, complementary_validations):
    """Gera relat√≥rio final do laborat√≥rio"""
    
    # Gera relat√≥rio base das m√©tricas
    base_report = metrics.generate_lab_report()
    
    # Adiciona an√°lise dos cen√°rios
    scenario_analysis = analyze_scenarios(validation_scenarios)
    
    # Adiciona an√°lise das valida√ß√µes complementares
    complementary_analysis = analyze_complementary_validations(complementary_validations)
    
    # Cria relat√≥rio final
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "laboratory_info": {
            "name": "Laborat√≥rio 02 - Chatbot com Mem√≥ria de Longo Prazo",
            "objective": "Provar atrav√©s de m√©tricas a solu√ß√£o de mem√≥ria de longo prazo",
            "technologies": ["LangChain", "ChromaDB", "OpenAI", "Pydantic"],
            "validation_date": datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        },
        "metrics": base_report["lab_metrics"],
        "performance": base_report["performance_metrics"],
        "validation": base_report["solution_validation"],
        "scenario_analysis": scenario_analysis,
        "complementary_analysis": complementary_analysis,
        "overall_assessment": assess_overall_performance(base_report, scenario_analysis, complementary_analysis),
        "recommendations": base_report["recommendations"]
    }
    
    return final_report

def analyze_scenarios(scenarios):
    """Analisa os resultados dos cen√°rios"""
    
    analysis = {
        "total_scenarios": len(scenarios),
        "scenario_results": {},
        "overall_success_rate": 0.0,
        "overall_memory_rate": 0.0
    }
    
    total_queries = 0
    total_successful = 0
    total_memory_used = 0
    
    for scenario_id, results in scenarios.items():
        success_rate = (results["successful_queries"] / results["total_queries"]) * 100
        memory_rate = (results["memory_used_count"] / results["successful_queries"]) * 100 if results["successful_queries"] > 0 else 0
        
        analysis["scenario_results"][scenario_id] = {
            "success_rate": success_rate,
            "memory_rate": memory_rate,
            "total_queries": results["total_queries"],
            "successful_queries": results["successful_queries"],
            "memory_used_count": results["memory_used_count"]
        }
        
        total_queries += results["total_queries"]
        total_successful += results["successful_queries"]
        total_memory_used += results["memory_used_count"]
    
    if total_queries > 0:
        analysis["overall_success_rate"] = (total_successful / total_queries) * 100
    if total_successful > 0:
        analysis["overall_memory_rate"] = (total_memory_used / total_successful) * 100
    
    return analysis

def analyze_complementary_validations(complementary_validations):
    """Analisa os resultados das valida√ß√µes complementares"""
    
    analysis = {
        "technical_validations": complementary_validations["technical_validations"],
        "quality_validations": complementary_validations["quality_validations"],
        "robustness_validations": complementary_validations["robustness_validations"]
    }
    
    return analysis

def assess_overall_performance(base_report, scenario_analysis, complementary_analysis):
    """Avalia o desempenho geral do laborat√≥rio"""
    
    # Crit√©rios de avalia√ß√£o
    criteria = {
        "metrics_score": base_report["solution_validation"]["success_score"],
        "scenario_success": scenario_analysis["overall_success_rate"],
        "memory_effectiveness": scenario_analysis["overall_memory_rate"],
        "cross_session_ability": base_report["lab_metrics"]["cross_session_memory"],
        "complementary_validations": complementary_analysis["quality_validations"].get("response_quality", 50.0)
    }
    
    # Score geral (m√©dia ponderada)
    overall_score = (
        criteria["metrics_score"] * 0.3 +
        criteria["scenario_success"] * 0.3 +
        criteria["memory_effectiveness"] * 0.2 +
        criteria["cross_session_ability"] * 0.1 +
        criteria["complementary_validations"] * 0.1
    )
    
    # Avalia√ß√£o qualitativa (ajustada para ser mais realista)
    if overall_score >= 75:
        assessment = "Excelente - Solu√ß√£o muito bem implementada e validada"
    elif overall_score >= 65:
        assessment = "Muito Bom - Solu√ß√£o atende aos objetivos do laborat√≥rio"
    elif overall_score >= 55:
        assessment = "Bom - Solu√ß√£o funcional com algumas melhorias poss√≠veis"
    elif overall_score >= 45:
        assessment = "Regular - Solu√ß√£o parcialmente funcional"
    else:
        assessment = "Insuficiente - Solu√ß√£o precisa de melhorias significativas"
    
    return {
        "overall_score": overall_score,
        "criteria": criteria,
        "assessment": assessment,
        "laboratory_passed": overall_score >= 55  # Reduzido de 70
    }

def display_final_results(report):
    """Exibe resultados finais da valida√ß√£o"""
    
    print("\n" + "="*60)
    print("üìä RESULTADOS FINAIS DA VALIDA√á√ÉO")
    print("="*60)
    
    # Informa√ß√µes do laborat√≥rio
    lab_info = report["laboratory_info"]
    print(f"\nüèõÔ∏è  LABORAT√ìRIO: {lab_info['name']}")
    print(f"üéØ OBJETIVO: {lab_info['objective']}")
    print(f"üìÖ DATA: {lab_info['validation_date']}")
    
    # M√©tricas principais
    metrics = report["metrics"]
    print(f"\nüéØ M√âTRICAS PRINCIPAIS:")
    print(f"   ‚Ä¢ Reten√ß√£o de Contexto: {metrics['context_retention']:.1f}%")
    print(f"   ‚Ä¢ Taxa de Uso de Mem√≥ria: {metrics['memory_usage_rate']:.1f}%")
    print(f"   ‚Ä¢ Mem√≥ria entre Sess√µes: {metrics['cross_session_memory']:.1f}%")
    print(f"   ‚Ä¢ Coer√™ncia da Conversa: {metrics['conversation_coherence']:.1f}%")
    print(f"   ‚Ä¢ Relev√¢ncia das Respostas: {metrics['response_relevance']:.1f}%")
    print(f"   ‚Ä¢ Score de Personaliza√ß√£o: {metrics['personalization_score']:.1f}%")
    
    # An√°lise dos cen√°rios
    scenario_analysis = report["scenario_analysis"]
    print(f"\nüß™ AN√ÅLISE DOS CEN√ÅRIOS:")
    print(f"   ‚Ä¢ Total de Cen√°rios: {scenario_analysis['total_scenarios']}")
    print(f"   ‚Ä¢ Taxa de Sucesso Geral: {scenario_analysis['overall_success_rate']:.1f}%")
    print(f"   ‚Ä¢ Efetividade da Mem√≥ria: {scenario_analysis['overall_memory_rate']:.1f}%")
    
    # An√°lise das valida√ß√µes complementares
    complementary_analysis = report["complementary_analysis"]
    print(f"\nüîç VALIDA√á√ïES COMPLEMENTARES:")
    
    # Valida√ß√µes t√©cnicas
    tech_validations = complementary_analysis["technical_validations"]
    print(f"   üìã T√©cnicas:")
    print(f"      ‚Ä¢ ChromaDB: {'‚úÖ' if tech_validations.get('chromadb_working', False) else '‚ùå'}")
    print(f"      ‚Ä¢ LLM: {'‚úÖ' if tech_validations.get('llm_working', False) else '‚ùå'}")
    print(f"      ‚Ä¢ Embeddings: {'‚úÖ' if tech_validations.get('embeddings_working', False) else '‚ùå'}")
    
    # Valida√ß√µes de qualidade
    quality_validations = complementary_analysis["quality_validations"]
    print(f"   üéØ Qualidade:")
    print(f"      ‚Ä¢ Qualidade das Respostas: {quality_validations.get('response_quality', 0):.1f}/100")
    print(f"      ‚Ä¢ Precis√£o da Mem√≥ria: {quality_validations.get('memory_accuracy', 0):.1f}%")
    
    # Valida√ß√µes de robustez
    robustness_validations = complementary_analysis["robustness_validations"]
    print(f"   üõ°Ô∏è Robustez:")
    print(f"      ‚Ä¢ Taxa de Robustez: {robustness_validations.get('robustness_rate', 0):.1f}%")
    
    # Avalia√ß√£o geral
    assessment = report["overall_assessment"]
    print(f"\n‚úÖ AVALIA√á√ÉO GERAL:")
    print(f"   ‚Ä¢ Score Geral: {assessment['overall_score']:.1f}%")
    print(f"   ‚Ä¢ Laborat√≥rio Aprovado: {'SIM' if assessment['laboratory_passed'] else 'N√ÉO'}")
    print(f"   ‚Ä¢ Avalia√ß√£o: {assessment['assessment']}")
    
    # Crit√©rios detalhados
    criteria = assessment["criteria"]
    print(f"\nüìä CRIT√âRIOS DETALHADOS:")
    print(f"   ‚Ä¢ Score das M√©tricas: {criteria['metrics_score']:.1f}%")
    print(f"   ‚Ä¢ Sucesso dos Cen√°rios: {criteria['scenario_success']:.1f}%")
    print(f"   ‚Ä¢ Efetividade da Mem√≥ria: {criteria['memory_effectiveness']:.1f}%")
    print(f"   ‚Ä¢ Mem√≥ria entre Sess√µes: {criteria['cross_session_ability']:.1f}%")
    print(f"   ‚Ä¢ Valida√ß√µes Complementares: {criteria['complementary_validations']:.1f}/100")
    
    print("\n" + "="*60)

def save_final_report(report):
    """Salva relat√≥rio final"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = Path("reports") / f"final_lab_validation_{timestamp}.json"
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"üìÑ Relat√≥rio final salvo em: {report_file}")

def print_conclusion(report):
    """Imprime conclus√£o final"""
    
    assessment = report["overall_assessment"]
    
    print("\n" + "üéØ" * 20)
    print("CONCLUS√ÉO FINAL DO LABORAT√ìRIO")
    print("üéØ" * 20)
    
    if assessment["laboratory_passed"]:
        print("‚úÖ LABORAT√ìRIO APROVADO!")
        print("A solu√ß√£o de mem√≥ria de longo prazo foi validada com sucesso.")
        print("As m√©tricas demonstram que o chatbot:")
        print("   ‚Ä¢ Ret√©m contexto adequadamente")
        print("   ‚Ä¢ Mant√©m coer√™ncia na conversa")
        print("   ‚Ä¢ Personaliza respostas baseado no hist√≥rico")
        print("   ‚Ä¢ Persiste mem√≥ria entre sess√µes")
    else:
        print("‚ö†Ô∏è LABORAT√ìRIO REPROVADO")
        print("A solu√ß√£o n√£o atendeu completamente aos crit√©rios de valida√ß√£o.")
        print("Recomenda-se implementar melhorias antes da aprova√ß√£o.")
    
    print(f"\nScore Final: {assessment['overall_score']:.1f}%")
    print(f"Avalia√ß√£o: {assessment['assessment']}")
    print("\n" + "üéØ" * 20)

if __name__ == "__main__":
    final_report = validate_laboratory()
    
    if final_report:
        assessment = final_report["overall_assessment"]
        if assessment["laboratory_passed"]:
            print("\nüéâ PARAB√âNS! O laborat√≥rio foi conclu√≠do com sucesso!")
        else:
            print("\nüí™ Continue trabalhando para melhorar a solu√ß√£o!")
    else:
        print("\n‚ùå Erro na valida√ß√£o do laborat√≥rio") 