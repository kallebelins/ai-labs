#!/usr/bin/env python3
"""
Gerador de relat√≥rios para o sistema RAG b√°sico
"""

import json
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime

class ReportGenerator:
    """Gerador de relat√≥rios para o sistema RAG"""
    
    def __init__(self, reports_dir: str = "reports"):
        """
        Inicializa o gerador de relat√≥rios
        
        Args:
            reports_dir: Diret√≥rio para salvar relat√≥rios
        """
        self.reports_dir = Path(reports_dir)
        self.reports_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_lab_report(self, metrics_summary: Dict[str, Any], system_info: Dict[str, Any]) -> str:
        """
        Gera relat√≥rio completo do laborat√≥rio
        
        Args:
            metrics_summary: Resumo das m√©tricas
            system_info: Informa√ß√µes do sistema
        
        Returns:
            Caminho do arquivo de relat√≥rio gerado
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.reports_dir / f"lab_report_{timestamp}.json"
        
        # Calcula score final
        final_score = self._calculate_final_score(metrics_summary)
        
        # Determina status do laborat√≥rio
        status = self._determine_lab_status(final_score)
        
        # Cria relat√≥rio
        report = {
            "lab_info": {
                "name": "Lab 3: RAG B√°sico",
                "description": "Sistema RAG b√°sico com recupera√ß√£o e gera√ß√£o",
                "timestamp": datetime.now().isoformat(),
                "status": status,
                "final_score": final_score
            },
            "system_info": system_info,
            "metrics_summary": metrics_summary,
            "detailed_metrics": {
                "context_recall": {
                    "value": metrics_summary.get("avg_context_recall", 0.0),
                    "target": 0.8,
                    "status": "‚úÖ" if metrics_summary.get("avg_context_recall", 0.0) >= 0.8 else "‚ùå"
                },
                "precision": {
                    "value": metrics_summary.get("avg_precision", 0.0),
                    "target": 0.75,
                    "status": "‚úÖ" if metrics_summary.get("avg_precision", 0.0) >= 0.75 else "‚ùå"
                },
                "rag_usage_rate": {
                    "value": metrics_summary.get("rag_usage_rate", 0.0),
                    "target": 90.0,
                    "status": "‚úÖ" if metrics_summary.get("rag_usage_rate", 0.0) >= 90.0 else "‚ùå"
                },
                "response_time": {
                    "value": metrics_summary.get("avg_response_time", 0.0),
                    "target": 3.0,
                    "status": "‚úÖ" if metrics_summary.get("avg_response_time", 0.0) <= 3.0 else "‚ùå"
                },
                "success_rate": {
                    "value": metrics_summary.get("success_rate", 0.0),
                    "target": 95.0,
                    "status": "‚úÖ" if metrics_summary.get("success_rate", 0.0) >= 95.0 else "‚ùå"
                }
            },
            "recommendations": self._generate_recommendations(metrics_summary),
            "conclusion": self._generate_conclusion(final_score, status)
        }
        
        # Salva relat√≥rio
        try:
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"üìä Relat√≥rio gerado: {report_file}")
            return str(report_file)
            
        except Exception as e:
            print(f"‚ùå Erro ao gerar relat√≥rio: {e}")
            return ""
    
    def _calculate_final_score(self, metrics_summary: Dict[str, Any]) -> float:
        """
        Calcula score final baseado nas m√©tricas
        
        Args:
            metrics_summary: Resumo das m√©tricas
        
        Returns:
            Score final (0-100)
        """
        if metrics_summary.get("total_queries", 0) == 0:
            return 0.0
        
        # Pesos para cada m√©trica
        weights = {
            "context_recall": 0.25,
            "precision": 0.20,
            "rag_usage": 0.20,
            "success_rate": 0.15,
            "response_time": 0.10,
            "confidence": 0.10
        }
        
        # Normaliza tempo de resposta (menor √© melhor)
        response_time = metrics_summary.get("avg_response_time", 0.0)
        response_time_score = max(0, 1 - (response_time / 5.0))  # 5s como refer√™ncia
        
        # Calcula score ponderado
        score = (
            metrics_summary.get("avg_context_recall", 0.0) * weights["context_recall"] +
            metrics_summary.get("avg_precision", 0.0) * weights["precision"] +
            (metrics_summary.get("rag_usage_rate", 0.0) / 100) * weights["rag_usage"] +
            (metrics_summary.get("success_rate", 0.0) / 100) * weights["success_rate"] +
            response_time_score * weights["response_time"] +
            metrics_summary.get("avg_confidence", 0.0) * weights["confidence"]
        )
        
        return score * 100  # Converte para percentual
    
    def _determine_lab_status(self, final_score: float) -> str:
        """
        Determina status do laborat√≥rio baseado no score
        
        Args:
            final_score: Score final do laborat√≥rio
        
        Returns:
            Status do laborat√≥rio
        """
        if final_score >= 90:
            return "LABORAT√ìRIO APROVADO"
        elif final_score >= 80:
            return "LABORAT√ìRIO APROVADO COM RESERVAS"
        elif final_score >= 70:
            return "LABORAT√ìRIO EM REVIS√ÉO"
        else:
            return "LABORAT√ìRIO REPROVADO"
    
    def _generate_recommendations(self, metrics_summary: Dict[str, Any]) -> List[str]:
        """
        Gera recomenda√ß√µes baseadas nas m√©tricas
        
        Args:
            metrics_summary: Resumo das m√©tricas
        
        Returns:
            Lista de recomenda√ß√µes
        """
        recommendations = []
        
        # An√°lise de recall de contexto
        context_recall = metrics_summary.get("avg_context_recall", 0.0)
        if context_recall < 0.8:
            recommendations.append(
                "Melhorar recall de contexto: Considere ajustar threshold de similaridade ou expandir base de documentos"
            )
        
        # An√°lise de precis√£o
        precision = metrics_summary.get("avg_precision", 0.0)
        if precision < 0.75:
            recommendations.append(
                "Melhorar precis√£o das respostas: Otimize prompts e refine crit√©rios de recupera√ß√£o"
            )
        
        # An√°lise de uso de RAG
        rag_usage = metrics_summary.get("rag_usage_rate", 0.0)
        if rag_usage < 90.0:
            recommendations.append(
                "Aumentar taxa de uso de RAG: Verifique se documentos est√£o sendo recuperados adequadamente"
            )
        
        # An√°lise de tempo de resposta
        response_time = metrics_summary.get("avg_response_time", 0.0)
        if response_time > 3.0:
            recommendations.append(
                "Otimizar tempo de resposta: Considere cache de embeddings ou otimiza√ß√£o de queries"
            )
        
        # An√°lise de taxa de sucesso
        success_rate = metrics_summary.get("success_rate", 0.0)
        if success_rate < 95.0:
            recommendations.append(
                "Melhorar taxa de sucesso: Verifique logs de erro e trate exce√ß√µes adequadamente"
            )
        
        if not recommendations:
            recommendations.append("Sistema funcionando adequadamente - mantenha monitoramento")
        
        return recommendations
    
    def _generate_conclusion(self, final_score: float, status: str) -> str:
        """
        Gera conclus√£o do relat√≥rio
        
        Args:
            final_score: Score final
            status: Status do laborat√≥rio
        
        Returns:
            Conclus√£o do relat√≥rio
        """
        if final_score >= 90:
            return (
                f"Excelente - Solu√ß√£o muito bem implementada e validada. "
                f"Score final de {final_score:.1f}% demonstra alta efetividade do sistema RAG."
            )
        elif final_score >= 80:
            return (
                f"Bom - Solu√ß√£o implementada com sucesso. "
                f"Score final de {final_score:.1f}% indica boa performance, com espa√ßo para melhorias."
            )
        elif final_score >= 70:
            return (
                f"Regular - Solu√ß√£o implementada mas requer melhorias. "
                f"Score final de {final_score:.1f}% indica necessidade de otimiza√ß√µes."
            )
        else:
            return (
                f"Insuficiente - Solu√ß√£o n√£o atende aos requisitos m√≠nimos. "
                f"Score final de {final_score:.1f}% indica necessidade de revis√£o completa."
            )
    
    def print_summary(self, metrics_summary: Dict[str, Any], system_info: Dict[str, Any]):
        """
        Imprime resumo das m√©tricas no console
        
        Args:
            metrics_summary: Resumo das m√©tricas
            system_info: Informa√ß√µes do sistema
        """
        print("\n" + "="*60)
        print("üìä RELAT√ìRIO DO SISTEMA RAG B√ÅSICO")
        print("="*60)
        
        # Informa√ß√µes do sistema
        print(f"\nü§ñ INFORMA√á√ïES DO SISTEMA:")
        print(f"   - Modelo: {system_info.get('model_name', 'N/A')}")
        print(f"   - Temperatura: {system_info.get('temperature', 'N/A')}")
        print(f"   - Modo de teste: {'Sim' if system_info.get('test_mode', False) else 'N√£o'}")
        print(f"   - Vetorstore dispon√≠vel: {'Sim' if system_info.get('vectorstore_available', False) else 'N√£o'}")
        print(f"   - Documentos: {system_info.get('documents_count', 0)}")
        
        # M√©tricas principais
        print(f"\nüìà M√âTRICAS PRINCIPAIS:")
        print(f"   - Total de queries: {metrics_summary.get('total_queries', 0)}")
        print(f"   - Taxa de sucesso: {metrics_summary.get('success_rate', 0.0):.1f}%")
        print(f"   - Tempo m√©dio de resposta: {metrics_summary.get('avg_response_time', 0.0):.2f}s")
        print(f"   - Tokens m√©dios por query: {metrics_summary.get('avg_tokens_per_query', 0.0):.1f}")
        
        # M√©tricas de qualidade
        print(f"\nüéØ M√âTRICAS DE QUALIDADE:")
        print(f"   - Recall de contexto: {metrics_summary.get('avg_context_recall', 0.0):.1%}")
        print(f"   - Precis√£o: {metrics_summary.get('avg_precision', 0.0):.1%}")
        print(f"   - Confian√ßa: {metrics_summary.get('avg_confidence', 0.0):.1%}")
        print(f"   - Taxa de uso de RAG: {metrics_summary.get('rag_usage_rate', 0.0):.1f}%")
        print(f"   - Documentos m√©dios usados: {metrics_summary.get('avg_documents_used', 0.0):.1f}")
        
        # Score final
        final_score = self._calculate_final_score(metrics_summary)
        status = self._determine_lab_status(final_score)
        
        print(f"\nüèÜ RESULTADO FINAL:")
        print(f"   - Score Final: {final_score:.1f}%")
        print(f"   - Status: {status}")
        
        print("="*60) 