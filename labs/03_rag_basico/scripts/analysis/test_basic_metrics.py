#!/usr/bin/env python3
"""
Teste de mÃ©tricas bÃ¡sicas do sistema RAG simplificado.

Este script testa as mÃ©tricas bÃ¡sicas do sistema RAG:
- Taxa de sucesso
- Tempo de resposta
- Recall e precisÃ£o bÃ¡sicos
"""

import os
import sys
import json
import time
from pathlib import Path

# Adiciona o diretÃ³rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.core.rag_system import RAGSystem
from src.utils.config import load_config
from src.utils.metrics import SimpleMetrics

def test_basic_metrics():
    """Testa mÃ©tricas bÃ¡sicas do sistema RAG"""
    
    print("ðŸ§ª TESTE DE MÃ‰TRICAS BÃSICAS")
    print("=" * 50)
    
    # Carrega configuraÃ§Ã£o
    config = load_config()
    config["test_mode"] = True
    
    # Inicializa sistema RAG
    rag_system = RAGSystem(config)
    
    # Inicializa coletor de mÃ©tricas
    metrics = SimpleMetrics()
    metrics.reset_metrics()
    
    # Queries de teste
    test_queries = [
        "O que Ã© inteligÃªncia artificial?",
        "Explique machine learning",
        "Como funciona deep learning?",
        "O que Ã© RAG?",
        "Defina embeddings"
    ]
    
    print(f"ðŸ“Š Executando {len(test_queries)} queries de teste...")
    
    results = []
    for i, query in enumerate(test_queries, 1):
        print(f"\n{i}. Processando: '{query}'")
        
        start_time = time.time()
        result = rag_system.process_query(query)
        result["response_time"] = time.time() - start_time
        
        # Registra mÃ©tricas
        metrics.record_query_result(result)
        results.append(result)
        
        # Mostra resultado
        if result["success"]:
            print(f"   âœ… Sucesso - {result['response_time']:.2f}s")
            print(f"   ðŸ“„ Documentos: {result['documents_used']}")
            print(f"   ðŸ“‹ Recall: {result['context_recall']:.2f}")
            print(f"   ðŸŽ¯ PrecisÃ£o: {result['precision']:.2f}")
        else:
            print(f"   âŒ Falha: {result.get('error_message', 'Erro desconhecido')}")
    
    # Mostra resumo das mÃ©tricas
    print("\n" + "=" * 50)
    print("ðŸ“Š RESUMO DAS MÃ‰TRICAS")
    print("=" * 50)
    
    summary = metrics.get_summary()
    final_score = metrics.calculate_final_score()
    
    print(f"Total de queries: {summary['total_queries']}")
    print(f"Taxa de sucesso: {summary['success_rate']:.1f}%")
    print(f"Tempo mÃ©dio: {summary['avg_response_time']:.2f}s")
    print(f"Recall mÃ©dio: {summary['avg_context_recall']:.2f}")
    print(f"PrecisÃ£o mÃ©dia: {summary['avg_precision']:.2f}")
    print(f"Taxa de uso RAG: {summary['rag_usage_rate']:.1f}%")
    print(f"\nðŸŽ¯ SCORE FINAL: {final_score:.1f}%")
    
    # Salva relatÃ³rio
    report = {
        "test_type": "basic_metrics",
        "timestamp": time.time(),
        "config": config,
        "results": results,
        "summary": summary,
        "final_score": final_score
    }
    
    report_file = Path("reports") / f"basic_metrics_test_{int(time.time())}.json"
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ’¾ RelatÃ³rio salvo: {report_file}")
    
    return final_score >= 70.0

if __name__ == "__main__":
    success = test_basic_metrics()
    sys.exit(0 if success else 1) 